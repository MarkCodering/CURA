{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dotenv\n",
    "dotenv.load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from langsmith import Client\n",
    "\n",
    "splits = {'dev': 'data/dev-00000-of-00001.parquet', 'test': 'data/test-00000-of-00001.parquet', 'train': 'data/train-00000-of-00001.parquet'}\n",
    "df = pd.read_parquet(\"hf://datasets/princeton-nlp/SWE-bench/\" + splits[\"test\"])\n",
    "df['version'] = df['version'].apply(lambda x: f\"version:{x}\")\n",
    "df.to_csv(\"data/SWE-bench-test.csv\",index=False)\n",
    "\n",
    "\n",
    "\n",
    "client = Client()\n",
    "dataset = client.upload_csv(\n",
    "    csv_file=\"data/SWE-bench-test.csv\",\n",
    "    input_keys=list(df.columns),\n",
    "    output_keys=[],\n",
    "    name=\"swe-bench-test\",\n",
    "    description=\"SWE-bench-test dataset\",\n",
    "    data_type=\"kv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith.evaluation import evaluate\n",
    "from langsmith import Client\n",
    "from prediction import do_prediction\n",
    "import random\n",
    "\n",
    "client = Client()\n",
    "\n",
    "def predict(inputs: dict):\n",
    "    return {\n",
    "        \"instance_id\":inputs['instance_id'],\n",
    "        \"model_patch\": do_prediction(inputs),\n",
    "        \"model_name_or_path\":\"test-model\"\n",
    "    }\n",
    "\n",
    "test_dataset = list(client.list_examples(dataset_id=\"9afe7b08-5fee-4338-8d8f-b0dbfb677307\"))\n",
    "random.seed(42)\n",
    "random_examples = random.sample(test_dataset, 10)\n",
    "\n",
    "result = evaluate(\n",
    "    predict,\n",
    "    data=random_examples,\n",
    ")\n",
    "print(result.experiment_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from swebench.harness.run_evaluation import run_instances\n",
    "import resource\n",
    "import docker\n",
    "from swebench.harness.docker_utils import list_images, clean_images\n",
    "from swebench.harness.docker_build import build_env_images\n",
    "from pathlib import Path\n",
    "import json\n",
    "import os\n",
    "from langsmith.evaluation import evaluate_existing\n",
    "from langsmith.schemas import Example, Run\n",
    "\n",
    "RUN_EVALUATION_LOG_DIR = Path(\"logs/run_evaluation\")\n",
    "LANGSMITH_EVALUATION_DIR = './langsmith_feedback/feedback.json'\n",
    "\n",
    "def convert_runs_to_langsmith_feedback(\n",
    "        predictions: dict,\n",
    "        full_dataset: list,\n",
    "        run_id: str\n",
    "    ) -> float:\n",
    "    \"\"\"\n",
    "    Convert logs from docker containers into LangSmith feedback.\n",
    "\n",
    "    Args:\n",
    "        predictions (dict): Predictions dict generated by the model\n",
    "        full_dataset (list): List of all instances\n",
    "        run_id (str): Run ID\n",
    "    \"\"\"\n",
    "    feedback_for_all_instances = {}\n",
    "\n",
    "    for instance in full_dataset:\n",
    "        feedback_for_instance = []\n",
    "        instance_id = instance['instance_id']\n",
    "        prediction = predictions[instance_id]\n",
    "        if prediction.get(\"model_patch\", None) in [\"\", None]:\n",
    "            # Prediction returned an empty patch\n",
    "            feedback_for_all_instances[prediction['run_id']] = [{\"key\":\"non-empty-patch\",\"score\":0},\n",
    "                                                                {\"key\":\"completed-patch\",\"score\":0},\n",
    "                                                                {\"key\":\"resolved-patch\",\"score\":0}]\n",
    "            continue\n",
    "        feedback_for_instance.append({\"key\":\"non-empty-patch\",\"score\":1})\n",
    "        report_file = (\n",
    "            RUN_EVALUATION_LOG_DIR\n",
    "            / run_id\n",
    "            / prediction[\"model_name_or_path\"].replace(\"/\", \"__\")\n",
    "            / prediction['instance_id']\n",
    "            / \"report.json\"\n",
    "        )\n",
    "        if report_file.exists():\n",
    "            # If report file exists, then the instance has been run\n",
    "            feedback_for_instance.append({\"key\":\"completed-patch\",\"score\":1})\n",
    "            report = json.loads(report_file.read_text())\n",
    "            # Check if instance actually resolved the PR\n",
    "            if report[instance_id][\"resolved\"]:\n",
    "                feedback_for_instance.append({\"key\":\"resolved-patch\",\"score\":1})\n",
    "            else:\n",
    "                feedback_for_instance.append({\"key\":\"resolved-patch\",\"score\":0})\n",
    "        else:\n",
    "            # The instance did not run succesfully\n",
    "            feedback_for_instance += [{\"key\":\"completed-patch\",\"score\":0},{\"key\":\"resolved-patch\",\"score\":0}]\n",
    "        feedback_for_all_instances[prediction['run_id']] = feedback_for_instance\n",
    "\n",
    "    os.makedirs(os.path.dirname(LANGSMITH_EVALUATION_DIR), exist_ok=True)\n",
    "    with open(LANGSMITH_EVALUATION_DIR, 'w') as json_file:\n",
    "        json.dump(feedback_for_all_instances, json_file)\n",
    "\n",
    "def evaluate_predictions(\n",
    "        dataset: list,\n",
    "        predictions: list,\n",
    "        max_workers: int,\n",
    "        force_rebuild: bool,\n",
    "        cache_level: str,\n",
    "        clean: bool,\n",
    "        open_file_limit: int,\n",
    "        run_id: str,\n",
    "        timeout: int,\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Run evaluation harness for the given dataset and predictions.\n",
    "    \"\"\"\n",
    "    # set open file limit\n",
    "    assert len(run_id) > 0, \"Run ID must be provided\"\n",
    "    resource.setrlimit(resource.RLIMIT_NOFILE, (open_file_limit, open_file_limit))\n",
    "    client = docker.from_env()\n",
    "\n",
    "    existing_images = list_images(client)\n",
    "    print(f\"Running {len(dataset)} unevaluated instances...\")\n",
    "    # build environment images + run instances\n",
    "    build_env_images(client, dataset, force_rebuild, max_workers)\n",
    "    run_instances(predictions, dataset, cache_level, clean, force_rebuild, max_workers, run_id, timeout)\n",
    "\n",
    "    # clean images + make final report\n",
    "    clean_images(client, existing_images, cache_level, clean)\n",
    "\n",
    "    convert_runs_to_langsmith_feedback(predictions,dataset,run_id)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "result = evaluate_existing(\"artistic-butter-12\")\n",
    "\n",
    "dataset = []\n",
    "predictions = {}\n",
    "for res in result:\n",
    "    predictions[res['run'].outputs['instance_id']] = {**res['run'].outputs,**{\"run_id\":str(res['run'].id)}}\n",
    "    dataset.append(res['run'].inputs['inputs'])\n",
    "for d in dataset:\n",
    "    d['version'] = d['version'].split(\":\")[1]\n",
    "\n",
    "evaluate_predictions(dataset,predictions,max_workers=8,force_rebuild=False,cache_level=\"env\",clean=False \\\n",
    "                     ,open_file_limit=4096,run_id=\"test\",timeout=1_800)\n",
    "\n",
    "\n",
    "\n",
    "def swe_bench_evaluator(run: Run, example: Example):\n",
    "    with open(LANGSMITH_EVALUATION_DIR, 'r') as json_file:\n",
    "        langsmith_eval = json.load(json_file)\n",
    "    return {\"results\": langsmith_eval[str(run.id)]}\n",
    "\n",
    "experiment_name = result.experiment_name\n",
    "evaluate_existing(experiment_name, evaluators=[swe_bench_evaluator])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
